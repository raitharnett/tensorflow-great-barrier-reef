{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cots-mask-r-nn.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "private_outputs": true,
      "mount_file_id": "1iD-Bq2HkObrtaqlIgCmPXzmtGa_cgH_N",
      "authorship_tag": "ABX9TyODFaaBARoDtNKEfd0CqH0y",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/raitharnett/tensorflow-great-barrier-reef/blob/main/cots_mask_r_nn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "# upgrade pip\n",
        "python -m pip install --upgrade pip\n",
        "# opencv\n",
        "pip uninstall --yes opencv_python\n",
        "pip install opencv-python-headless\n",
        "# TF Object detection\n",
        "git clone https://github.com/tensorflow/models.git \n",
        "cd models/research\n",
        "# Compile protos.\n",
        "protoc object_detection/protos/*.proto --python_out=.\n",
        "# Install TensorFlow Object Detection API.\n",
        "cp object_detection/packages/tf2/setup.py .\n",
        "python -m pip install --use-feature=2020-resolver .\n",
        "python object_detection/builders/model_builder_tf2_test.py"
      ],
      "metadata": {
        "id": "ZhL0rZMjBWug"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "# COTS data\n",
        "unzip -d /content/tensorflow-great-barrier-reef /content/drive/MyDrive/cots/tensorflow-great-barrier-reef.zip\n",
        "# TF mask_rcnn model\n",
        "wget http://download.tensorflow.org/models/object_detection/tf2/20200711/mask_rcnn_inception_resnet_v2_1024x1024_coco17_gpu-8.tar.gz \n",
        "tar -xvzf mask_rcnn_inception_resnet_v2_1024x1024_coco17_gpu-8.tar.gz \n"
      ],
      "metadata": {
        "id": "fHVHJVwAjnnh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9IysfaTBTNOa"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import pathlib\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import io\n",
        "import scipy.misc\n",
        "import shutil\n",
        "import numpy as np\n",
        "from six import BytesIO\n",
        "from PIL import Image, ImageDraw, ImageFont\n",
        "from six.moves.urllib.request import urlopen\n",
        "\n",
        "import cv2 as cv \n",
        "from google.colab.patches import cv2_imshow\n",
        "import matplotlib.patches as patches\n",
        "print(f\"OpenCV version: {cv.__version__}\")\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "from object_detection.utils import dataset_util\n",
        "from object_detection.dataset_tools import tf_record_creation_util\n",
        "from object_detection.utils import ops as utils_ops\n",
        "from object_detection.utils import label_map_util\n",
        "from object_detection.utils import visualization_utils as vis_util\n",
        "from sklearn.model_selection import train_test_split\n",
        "import contextlib2\n",
        "from pathlib import Path\n",
        "from enum import Enum\n",
        "\n",
        "class COTSClass(Enum):\n",
        "  COTS = 1\n",
        "  \n",
        "%matplotlib inline\n",
        "tf.get_logger().setLevel('ERROR')\n",
        "print(f\"TF version: {tf.__version__}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# path constants\n",
        "COTS_DATA_ROOT = '/content/'\n",
        "COTS_DATA =  os.path.join(COTS_DATA_ROOT,'tensorflow-great-barrier-reef')\n",
        "COTS_DATA_IMAGES = os.path.join(COTS_DATA,'train_images')\n",
        "COTS_DATASET = '/content/dataset'\n",
        "Path(COTS_DATASET).mkdir(parents=True, exist_ok=True)\n",
        "COTS_DATA_TRAIN_TF_RECORDS = f'{COTS_DATASET}/train'\n",
        "COTS_DATA_TEST_TF_RECORDS = f'{COTS_DATASET}/test'"
      ],
      "metadata": {
        "id": "7K2ARTW8-twE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load COTS data\n",
        "def cots_annotations(data):\n",
        "  import json\n",
        "  return json.loads(data.replace(\"'\", '\"'))\n",
        "\n",
        "def cots_image_path(row):\n",
        "  return  os.path.join(COTS_DATA_IMAGES,\n",
        "                       f\"video_{row.video_id}\",\n",
        "                       f\"{row.video_frame}.jpg\")\n",
        "\n",
        "cots_df = pd.read_csv(os.path.join(COTS_DATA,'train.csv'), converters={'annotations':cots_annotations})\n",
        "cots_df['image_path'] = cots_df.apply(cots_image_path, axis=1)\n",
        "cots_train_test_split = train_test_split(cots_df, train_size = 0.8)"
      ],
      "metadata": {
        "id": "JT7M7hELeL6x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "FORMAT = 'jpeg'.encode('utf8')\n",
        "CLASS_NAME = COTSClass.COTS.name.encode('utf8')\n",
        "\n",
        "def create_cots_tf_example(row): \n",
        "  contents = tf.io.read_file(row.image_path)\n",
        "  image = tf.io.decode_jpeg(contents, channels=3)\n",
        "  h,w,_ = image.shape\n",
        "  filename = row.image_id.encode('utf8')\n",
        "  boxes = np.array([[a['x'], a['x'] + a['width'], a['y'], a['y'] + a['height']] for a in row.annotations], dtype='float64')\n",
        "  xmin, xmax, ymin, ymax, classes_text, classes = [], [], [], [], [], []\n",
        "  if  (0 < boxes.size):\n",
        "    # normalize\n",
        "    boxes[:,[0,1]] *= 1/w\n",
        "    boxes[:,[2,3]] *= 1/h\n",
        "    xmin, xmax = np.transpose(boxes[:,[0,1]]).tolist()\n",
        "    ymin, ymax = np.transpose(boxes[:,[2,3]]).tolist()\n",
        "    classes_text = [CLASS_NAME for i in range(len(xmin))] \n",
        "    classes = [COTSClass.COTS.value for i in range(len(xmin))]\n",
        "  \n",
        "  feature = { 'image/height': dataset_util.int64_feature(h),\n",
        "              'image/width': dataset_util.int64_feature(w),\n",
        "              'image/filename': dataset_util.bytes_feature(filename),\n",
        "              'image/source_id': dataset_util.bytes_feature(filename),\n",
        "              'image/encoded': dataset_util.bytes_feature(contents.numpy()),\n",
        "              'image/format': dataset_util.bytes_feature(FORMAT),\n",
        "              'image/object/bbox/xmin': dataset_util.float_list_feature(xmin),\n",
        "              'image/object/bbox/xmax': dataset_util.float_list_feature(xmax),\n",
        "              'image/object/bbox/ymin': dataset_util.float_list_feature(ymin),\n",
        "              'image/object/bbox/ymax': dataset_util.float_list_feature(ymax),\n",
        "              'image/object/class/text': dataset_util.bytes_list_feature(classes_text),\n",
        "              'image/object/class/label': dataset_util.int64_list_feature(classes),\n",
        "            }\n",
        "  return tf.train.Example(features=tf.train.Features(feature=feature))\n",
        "\n",
        "def load_cots(df, base_path, num_shards=10):\n",
        "  with contextlib2.ExitStack() as stack:\n",
        "    records = tf_record_creation_util.open_sharded_output_tfrecords(stack, base_path, num_shards)\n",
        "    for index, row in df.iterrows():\n",
        "      example = create_cots_tf_example(row)\n",
        "      shard_index = index % num_shards\n",
        "      records[shard_index].write(example.SerializeToString())"
      ],
      "metadata": {
        "id": "364dxDVIGRwV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cots_tf_record_keys = [COTS_DATA_TRAIN_TF_RECORDS, COTS_DATA_TEST_TF_RECORDS]\n",
        "cots_train_test_data = {cots_tf_record_keys[i]: cots_train_test_split[i] for i in range(len(cots_train_test_split))}"
      ],
      "metadata": {
        "id": "9ATsQdxxr6AX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for base_path, df in cots_train_test_data.items():\n",
        "  load_cots(df, base_path)"
      ],
      "metadata": {
        "id": "xzlrEqifbGNP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from string import Template\n",
        "\n",
        "TRAINING_STEPS = 1000\n",
        "WARMUP_STEPS = 100\n",
        "BATCH_SIZE = 2\n",
        "\n",
        "MODEL_LABEL_MAP = \"/content/drive/MyDrive/cots/label_map.txt\"\n",
        "MODEL_PIPELINE_CONFIG_TEMPLATE = \"/content/drive/MyDrive/cots/mask_rcnn_inception_resnet_v2_1024x1024_coco17_gpu-8.config\"\n",
        "\n",
        "\n",
        "COTS_MODEL_DIR='/content/drive/MyDrive/cots/mask_rcnn_inception_resnet_v2'\n",
        "\n",
        "PIPELINE_CONFIG_PATH = '/content/dataset/pipeline.config'\n",
        "LABEL_MAP_PATH = '/content/dataset/label_map.pbtxt'\n",
        "\n",
        "shutil.copy(MODEL_LABEL_MAP, LABEL_MAP_PATH)\n",
        "\n",
        "category_index = label_map_util.create_category_index_from_labelmap(LABEL_MAP_PATH, use_display_name=True)\n",
        "\n",
        "with open(MODEL_PIPELINE_CONFIG_TEMPLATE, mode='r') as f:\n",
        "  config_file_template = f.read()\n",
        "pipeline = Template(config_file_template).substitute(training_steps=TRAINING_STEPS, warmup_steps=WARMUP_STEPS, batch_size=BATCH_SIZE)\n",
        "with open(PIPELINE_CONFIG_PATH, mode='w') as f:\n",
        "  f.write(pipeline)"
      ],
      "metadata": {
        "id": "FK6GbHB8wwPT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash -s \"$COTS_MODEL_DIR\" \"$PIPELINE_CONFIG_PATH\"\n",
        "MODEL_DIR=$1\n",
        "PIPELINE_CONFIG_PATH=$2\n",
        "# train model - does not work yet? \n",
        "python models/research/object_detection/model_main_tf2.py \\\n",
        "    --pipeline_config_path=$PIPELINE_CONFIG_PATH \\\n",
        "    --model_dir=$MODEL_DIR \\\n",
        "    --alsologtostderr\n",
        "# # evaluate model\n",
        "# python models/research/object_detection/model_main_tf2.py \\\n",
        "#     --pipeline_config_path=$PIPELINE_CONFIG_PATH \\\n",
        "#     --model_dir=$MODEL_DIR \\\n",
        "#     --checkpoint_dir=$MODEL_DIR \\\n",
        "#     --eval_timeout=0 \\\n",
        "#     --alsologtostderr\n",
        "# # save model\n",
        "# python models/research/object_detection/exporter_main_v2.py \\\n",
        "#     --input_type image_tensor \\\n",
        "#     --pipeline_config_path=$PIPELINE_CONFIG_PATH \\\n",
        "#     --trained_checkpoint_dir=$MODEL_DIR \\\n",
        "#     --output_directory=$MODEL_DIR/output \\\n",
        "#     --alsologtostderr"
      ],
      "metadata": {
        "id": "UK6ZwU9V92hZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "detector = hub.load(\"https://tfhub.dev/tensorflow/mask_rcnn/inception_resnet_v2_1024x1024/1\")"
      ],
      "metadata": {
        "id": "IITzpM1ngUKr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "PATH_TO_LABELS = './models/research/object_detection/data/mscoco_label_map.pbtxt'\n",
        "category_index = label_map_util.create_category_index_from_labelmap(PATH_TO_LABELS, use_display_name=True)\n",
        "cots_df_elements = cots_df[0 < cots_df['annotations'].map(len)].sample(n=1)\n",
        "\n",
        "for _, row in cots_df_elements.iterrows():\n",
        "  image_tensor = tf.io.decode_jpeg(tf.io.read_file(row.image_path), channels=3)\n",
        "  detector_output = detector(image_tensor[tf.newaxis, ...])\n",
        "  result = {key:value.numpy() for key,value in detector_output.items()}\n",
        "  label_id_offset = 0\n",
        "  image_np =  image_tensor.numpy()\n",
        "  image_np_with_mask = image_tensor.numpy().copy()\n",
        "  if 'detection_masks' in result:\n",
        "    # we need to convert np.arrays to tensors\n",
        "    detection_masks = tf.convert_to_tensor(result['detection_masks'][0])\n",
        "    detection_boxes = tf.convert_to_tensor(result['detection_boxes'][0])\n",
        "\n",
        "    # Reframe the bbox mask to the image size.\n",
        "    detection_masks_reframed = utils_ops.reframe_box_masks_to_image_masks(\n",
        "              detection_masks, detection_boxes,\n",
        "                image_np.shape[0], image_np.shape[1])\n",
        "    detection_masks_reframed = tf.cast(detection_masks_reframed > 0.5,\n",
        "                                        tf.uint8)\n",
        "    result['detection_masks_reframed'] = detection_masks_reframed.numpy()\n",
        "\n",
        "  vis_util.visualize_boxes_and_labels_on_image_array(\n",
        "        image_np_with_mask,\n",
        "        result['detection_boxes'][0],\n",
        "        (result['detection_classes'][0] + label_id_offset).astype(int),\n",
        "        result['detection_scores'][0],\n",
        "        category_index,\n",
        "        use_normalized_coordinates=True,\n",
        "        max_boxes_to_draw=200,\n",
        "        min_score_thresh=.30,\n",
        "        agnostic_mode=False,\n",
        "        instance_masks=result.get('detection_masks_reframed', None),\n",
        "        line_thickness=8)\n",
        "  \n",
        "  fig, ax = plt.subplots(figsize=(24,32))\n",
        "  plt.imshow(image_np_with_mask)\n",
        "  for a in row.annotations:\n",
        "    ax.add_patch(patches.Rectangle((a['x'], a['y']),  a['width'], a['height'], linewidth=3, edgecolor='r', facecolor='none'))\n",
        "  plt.show()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "HpunbSQF4RNq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config = {\n",
        "    \"sigma_list\": [15, 80, 250],\n",
        "    \"G\"         : 5.0,\n",
        "    \"b\"         : 25.0,\n",
        "    \"alpha\"     : 125.0,\n",
        "    \"beta\"      : 46.0,\n",
        "    \"low_clip\"  : 0.01,\n",
        "    \"high_clip\" : 0.99\n",
        "}\n",
        "def singleScaleRetinex(img, sigma):\n",
        "\n",
        "    retinex = np.log10(img) - np.log10(cv.GaussianBlur(img, (0, 0), sigma))\n",
        "\n",
        "    return retinex\n",
        "\n",
        "def multiScaleRetinex(img, sigma_list):\n",
        "\n",
        "    retinex = np.zeros_like(img)\n",
        "    for sigma in sigma_list:\n",
        "        retinex += singleScaleRetinex(img, sigma)\n",
        "\n",
        "    retinex = retinex / len(sigma_list)\n",
        "\n",
        "    return retinex\n",
        "\n",
        "def colorRestoration(img, alpha, beta):\n",
        "\n",
        "    img_sum = np.sum(img, axis=2, keepdims=True)\n",
        "\n",
        "    color_restoration = beta * (np.log10(alpha * img) - np.log10(img_sum))\n",
        "\n",
        "    return color_restoration\n",
        "\n",
        "def simplestColorBalance(img, low_clip, high_clip):    \n",
        "\n",
        "    total = img.shape[0] * img.shape[1]\n",
        "    for i in range(img.shape[2]):\n",
        "        unique, counts = np.unique(img[:, :, i], return_counts=True)\n",
        "        current = 0\n",
        "        for u, c in zip(unique, counts):            \n",
        "            if float(current) / total < low_clip:\n",
        "                low_val = u\n",
        "            if float(current) / total < high_clip:\n",
        "                high_val = u\n",
        "            current += c\n",
        "                \n",
        "        img[:, :, i] = np.maximum(np.minimum(img[:, :, i], high_val), low_val)\n",
        "\n",
        "    return img    \n",
        "\n",
        "def MSRCR(img, sigma_list, G, b, alpha, beta, low_clip, high_clip):\n",
        "\n",
        "    img = np.float64(img) + 1.0\n",
        "\n",
        "    img_retinex = multiScaleRetinex(img, sigma_list)    \n",
        "    img_color = colorRestoration(img, alpha, beta)    \n",
        "    img_msrcr = G * (img_retinex * img_color + b)\n",
        "\n",
        "    for i in range(img_msrcr.shape[2]):\n",
        "        img_msrcr[:, :, i] = (img_msrcr[:, :, i] - np.min(img_msrcr[:, :, i])) / \\\n",
        "                             (np.max(img_msrcr[:, :, i]) - np.min(img_msrcr[:, :, i])) * \\\n",
        "                             255\n",
        "    \n",
        "    img_msrcr = np.uint8(np.minimum(np.maximum(img_msrcr, 0), 255))\n",
        "    img_msrcr = simplestColorBalance(img_msrcr, low_clip, high_clip)       \n",
        "\n",
        "    return img_msrcr\n",
        "\n",
        "def automatedMSRCR(img, sigma_list):\n",
        "\n",
        "    img = np.float64(img) + 1.0\n",
        "\n",
        "    img_retinex = multiScaleRetinex(img, sigma_list)\n",
        "\n",
        "    for i in range(img_retinex.shape[2]):\n",
        "        unique, count = np.unique(np.int32(img_retinex[:, :, i] * 100), return_counts=True)\n",
        "        for u, c in zip(unique, count):\n",
        "            if u == 0:\n",
        "                zero_count = c\n",
        "                break\n",
        "            \n",
        "        low_val = unique[0] / 100.0\n",
        "        high_val = unique[-1] / 100.0\n",
        "        for u, c in zip(unique, count):\n",
        "            if u < 0 and c < zero_count * 0.1:\n",
        "                low_val = u / 100.0\n",
        "            if u > 0 and c < zero_count * 0.1:\n",
        "                high_val = u / 100.0\n",
        "                break\n",
        "            \n",
        "        img_retinex[:, :, i] = np.maximum(np.minimum(img_retinex[:, :, i], high_val), low_val)\n",
        "        \n",
        "        img_retinex[:, :, i] = (img_retinex[:, :, i] - np.min(img_retinex[:, :, i])) / \\\n",
        "                               (np.max(img_retinex[:, :, i]) - np.min(img_retinex[:, :, i])) \\\n",
        "                               * 255\n",
        "\n",
        "    img_retinex = np.uint8(img_retinex)\n",
        "        \n",
        "    return img_retinex\n",
        "\n",
        "def MSRCP(img, sigma_list, low_clip, high_clip):\n",
        "\n",
        "    img = np.float64(img) + 1.0\n",
        "\n",
        "    intensity = np.sum(img, axis=2) / img.shape[2]    \n",
        "\n",
        "    retinex = multiScaleRetinex(intensity, sigma_list)\n",
        "\n",
        "    intensity = np.expand_dims(intensity, 2)\n",
        "    retinex = np.expand_dims(retinex, 2)\n",
        "\n",
        "    intensity1 = simplestColorBalance(retinex, low_clip, high_clip)\n",
        "\n",
        "    intensity1 = (intensity1 - np.min(intensity1)) / \\\n",
        "                 (np.max(intensity1) - np.min(intensity1)) * \\\n",
        "                 255.0 + 1.0\n",
        "\n",
        "    img_msrcp = np.zeros_like(img)\n",
        "    \n",
        "    for y in range(img_msrcp.shape[0]):\n",
        "        for x in range(img_msrcp.shape[1]):\n",
        "            B = np.max(img[y, x])\n",
        "            A = np.minimum(256.0 / B, intensity1[y, x, 0] / intensity[y, x, 0])\n",
        "            img_msrcp[y, x, 0] = A * img[y, x, 0]\n",
        "            img_msrcp[y, x, 1] = A * img[y, x, 1]\n",
        "            img_msrcp[y, x, 2] = A * img[y, x, 2]\n",
        "\n",
        "    img_msrcp = np.uint8(img_msrcp - 1.0)\n",
        "\n",
        "    return img_msrcp"
      ],
      "metadata": {
        "id": "Z6wUwr9QLxn1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cots_df_elements = cots_df[10 == cots_df['annotations'].map(len)].sample(n=1)\n",
        "for _, row in cots_df_elements.iterrows():\n",
        "  img = cv.imread(row.image_path)\n",
        "  img = MSRCR(img,config['sigma_list'],\n",
        "                     config['G'],\n",
        "                     config['b'],\n",
        "                     config['alpha'],\n",
        "                     config['beta'],\n",
        "                     config['low_clip'],\n",
        "                     config['high_clip'])\n",
        "  gray = cv.cvtColor(img,cv.COLOR_BGR2GRAY)\n",
        "  # gray = cv.GaussianBlur(gray,(3,3),0)\n",
        "  # thresh = cv.adaptiveThreshold(gray, 255, cv.ADAPTIVE_THRESH_GAUSSIAN_C, cv.THRESH_BINARY_INV, 5, 0)\n",
        "  overlay = img.copy()\n",
        "  for a in row.annotations:\n",
        "    roi = gray[a['y']:a['y']+a['height'],a['x']:a['x']+a['width']]\n",
        "    roi_img = img[a['y']:a['y']+a['height'],a['x']:a['x']+a['width']]\n",
        "    cv2_imshow(roi)\n",
        "    cv2_imshow(roi_img)\n",
        "    # global thresholding\n",
        "    ret1,th1 = cv.threshold(roi,127,255,cv.THRESH_BINARY)\n",
        "    # Otsu's thresholding\n",
        "    ret2,th2 = cv.threshold(roi,0,255,cv.THRESH_BINARY+cv.THRESH_OTSU)\n",
        "    # Otsu's thresholding after Gaussian filtering\n",
        "    blur = cv.GaussianBlur(roi,(5,5),0)\n",
        "    ret3,th3 = cv.threshold(blur,0,255,cv.THRESH_BINARY+cv.THRESH_OTSU)\n",
        "    ret4,th4 = cv.threshold(blur,0,255,cv.THRESH_TOZERO+cv.THRESH_OTSU)\n",
        "    ret5,th5 = cv.threshold(blur,0,255,cv.THRESH_TOZERO_INV+cv.THRESH_OTSU)\n",
        "    ret6,th6 = cv.threshold(blur,0,255,cv.THRESH_BINARY_INV+cv.THRESH_OTSU)\n",
        "    kernel = np.ones((3,3),np.uint8)\n",
        "    # th3 = cv.erode(th3,kernel,iterations = 1)\n",
        "    # th3 = cv.morphologyEx(th3, cv.MORPH_OPEN, kernel)\n",
        "\n",
        "    cv2_imshow(th1)\n",
        "    cv2_imshow(th2)\n",
        "    cv2_imshow(th3)\n",
        "    cv2_imshow(th4)\n",
        "    cv2_imshow(th5)\n",
        "    cv2_imshow(th6)\n",
        "\n",
        "    contours, hierarchy = cv.findContours(th3, cv.RETR_CCOMP , cv.CHAIN_APPROX_NONE,offset=(a['x'], a['y']))\n",
        "    contour = max(contours, key = lambda c : cv.arcLength(c, True))\n",
        "    # contour = max(contours, key = lambda c : cv.contourArea(c))\n",
        "    epsilon = 0.01 * cv.arcLength(contour, True)\n",
        "    approx_polygon = cv.approxPolyDP(contour, epsilon, True)\n",
        "    cv.drawContours(overlay, [approx_polygon], 0, (0,255,0), 3)\n",
        "  alpha = 0.25\n",
        "  img = cv.addWeighted(overlay, alpha, img, 1 - alpha, 0)\n",
        "  cv2_imshow(img)\n",
        "  cv.destroyAllWindows()\n"
      ],
      "metadata": {
        "id": "rfcTwBZct5wx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "YaN44dT1YMHk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cots_df_elements = cots_df[10 == cots_df['annotations'].map(len)].sample(n=1)\n",
        "for _, row in cots_df_elements.iterrows():\n",
        "  img = cv.imread(row.image_path)\n",
        "  img = MSRCR(img,config['sigma_list'],\n",
        "                     config['G'],\n",
        "                     config['b'],\n",
        "                     config['alpha'],\n",
        "                     config['beta'],\n",
        "                     config['low_clip'],\n",
        "                     config['high_clip'])\n",
        "  img_gray = cv.cvtColor(img,cv.COLOR_BGR2GRAY)\n",
        "  overlay = img.copy()\n",
        "  for a in row.annotations:\n",
        "    roi = img_gray[a['y']:a['y']+a['height'],a['x']:a['x']+a['width']]\n",
        "    blur = cv.GaussianBlur(roi,(5,5),0)\n",
        "    _, thr = cv.threshold(blur,0,255,cv.THRESH_BINARY+cv.THRESH_OTSU)\n",
        "    _, thr_inv = cv.threshold(blur,0,255,cv.THRESH_BINARY_INV+cv.THRESH_OTSU)\n",
        "    contours, _ = cv.findContours(thr, cv.RETR_CCOMP, cv.CHAIN_APPROX_NONE,offset=(a['x'], a['y']))\n",
        "    contours_inv, _ = cv.findContours(thr_inv, cv.RETR_CCOMP, cv.CHAIN_APPROX_NONE,offset=(a['x'], a['y']))\n",
        "    contour = max(contours+contours_inv, key = lambda c : cv.arcLength(c, True))\n",
        "    epsilon = 0.001 * cv.arcLength(contour, True)\n",
        "    approx_polygon = cv.approxPolyDP(contour, epsilon, True)\n",
        "    cv.drawContours(overlay, [approx_polygon], 0, (0,255,0), thickness=cv.FILLED)\n",
        "    # roi_img = img[a['y']:a['y']+a['height'],a['x']:a['x']+a['width']]\n",
        "    # cv2_imshow(roi)\n",
        "    # cv2_imshow(roi_img)\n",
        "    # # global thresholding\n",
        "    # ret1,th1 = cv.threshold(roi,127,255,cv.THRESH_BINARY)\n",
        "    # # Otsu's thresholding\n",
        "    # ret2,th2 = cv.threshold(roi,0,255,cv.THRESH_BINARY+cv.THRESH_OTSU)\n",
        "    # # Otsu's thresholding after Gaussian filtering\n",
        "    # blur = cv.GaussianBlur(roi,(5,5),0)\n",
        "    # ret3,th3 = cv.threshold(blur,0,255,cv.THRESH_BINARY+cv.THRESH_OTSU)\n",
        "    # ret4,th4 = cv.threshold(blur,0,255,cv.THRESH_TOZERO+cv.THRESH_OTSU)\n",
        "    # ret5,th5 = cv.threshold(blur,0,255,cv.THRESH_TOZERO_INV+cv.THRESH_OTSU)\n",
        "    # ret6,th6 = cv.threshold(blur,0,255,cv.THRESH_BINARY_INV+cv.THRESH_OTSU)\n",
        "    # kernel = np.ones((3,3),np.uint8)\n",
        "    # # th3 = cv.erode(th3,kernel,iterations = 1)\n",
        "    # # th3 = cv.morphologyEx(th3, cv.MORPH_OPEN, kernel)\n",
        "\n",
        "    # cv2_imshow(th1)\n",
        "    # cv2_imshow(th2)\n",
        "    # cv2_imshow(th3)\n",
        "    # cv2_imshow(th4)\n",
        "    # cv2_imshow(th5)\n",
        "    # cv2_imshow(th6)\n",
        "\n",
        "    # contours, hierarchy = cv.findContours(th3, cv.RETR_CCOMP , cv.CHAIN_APPROX_NONE,offset=(a['x'], a['y']))\n",
        "    # contour = max(contours, key = lambda c : cv.arcLength(c, True))\n",
        "    # # contour = max(contours, key = lambda c : cv.contourArea(c))\n",
        "    # epsilon = 0.01 * cv.arcLength(contour, True)\n",
        "    # approx_polygon = cv.approxPolyDP(contour, epsilon, True)\n",
        "    # cv.drawContours(overlay, [approx_polygon], 0, (0,255,0), 3)\n",
        "  alpha = 0.5\n",
        "  img = cv.addWeighted(overlay, alpha, img, 1 - alpha, 0)\n",
        "  cv2_imshow(img)\n",
        "  cv.destroyAllWindows()\n"
      ],
      "metadata": {
        "id": "v4oKqouWYNGI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cots_df_elements = cots_df[10 == cots_df['annotations'].map(len)].sample(n=1)\n",
        "for _, row in cots_df_elements.iterrows():\n",
        "  img = cv.imread(row.image_path)\n",
        "  img = MSRCR(img,config['sigma_list'],\n",
        "                     config['G'],\n",
        "                     config['b'],\n",
        "                     config['alpha'],\n",
        "                     config['beta'],\n",
        "                     config['low_clip'],\n",
        "                     config['high_clip'])\n",
        "  gray = cv.cvtColor(img,cv.COLOR_BGR2GRAY)\n",
        "  # thresh = cv.adaptiveThreshold(gray, 255, cv.ADAPTIVE_THRESH_GAUSSIAN_C, cv.THRESH_BINARY_INV, 5, 0)\n",
        "  overlay = img.copy()\n",
        "  for a in row.annotations:\n",
        "    roi = thresh[a['y']:a['y']+a['height'],a['x']:a['x']+a['width']]\n",
        "\n",
        "    kernel_size = 5\n",
        "    blur_roi = cv.GaussianBlur(roi,(kernel_size, kernel_size),0)\n",
        "    low_threshold = 50\n",
        "    high_threshold = 150\n",
        "    roi_edges = cv.Canny(blur_roi, low_threshold, high_threshold)\n",
        "    rho = 1  # distance resolution in pixels of the Hough grid\n",
        "    theta = np.pi / 180  # angular resolution in radians of the Hough grid\n",
        "    threshold = 15  # minimum number of votes (intersections in Hough grid cell)\n",
        "    min_line_length = 50  # minimum number of pixels making up a line\n",
        "    max_line_gap = 20  # maximum gap in pixels between connectable line segments\n",
        "    line_image = np.copy(img) * 0  # creating a blank to draw lines on\n",
        "\n",
        "    # Run Hough on edge detected image\n",
        "    # Output \"lines\" is an array containing endpoints of detected line segments\n",
        "    roi_lines = cv.HoughLinesP(roi_edges, rho, theta, threshold, np.array([]),\n",
        "                        min_line_length, max_line_gap)\n",
        "    for line in roi_lines:\n",
        "        for x1,y1,x2,y2 in line:\n",
        "          cv.line(overlay,(x1,y1),(x2,y2),(255,0,0),5)\n",
        "\n",
        "    # contours, hierarchy = cv.findContours(roi, cv.RETR_CCOMP , cv.CHAIN_APPROX_NONE,offset=(a['x'], a['y']))\n",
        "    # contour = max(contours, key = lambda c : cv.arcLength(c, True))\n",
        "    # epsilon = 0.01 * cv.arcLength(contour, True)\n",
        "    # approx_polygon = cv.approxPolyDP(contour, epsilon, True)\n",
        "    # cv.drawContours(overlay, [approx_polygon], 0, (0,255,0), thickness=cv.FILLED)\n",
        "  alpha = 0.25\n",
        "  img = cv.addWeighted(overlay, alpha, img, 1 - alpha, 0)\n",
        "  cv2_imshow(img)\n",
        "  cv.destroyAllWindows()"
      ],
      "metadata": {
        "id": "kHKx9plXbMQJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}